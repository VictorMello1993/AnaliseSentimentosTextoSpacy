# -*- coding: utf-8 -*-
"""Introdução ao spaCy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W97WOylgQqBVlV6Z5HqIVym6Ucn3wyNG

## Introdução ao spaCy

## Etapa 1: Instalação do spaCy
"""

!pip install spacy --upgrade
# !pip install spacy==2.3

import spacy
spacy.__version__

!python -m spacy download pt

"""## Etapa 2: Marcação POS



*   POS (part-of-speech) atribui para as palavras partes da fala (classes gramaticais), como substantivos, adjetivos, verbos;
*   Importante para a detecção de entidades no texto, pois primeiro é necessário saber o que o texto contém;
*   Lista de tokens https://spacy.io/api/annotation#pos-tagging
*   POS em português  https://www.sketchengine.eu/portuguese-freeling-part-of-speech-tagset/


### Legenda
*   lemma: raiz da palavra
*   pos: parte da fala
*   tag: informações morfológicas, como se o verbo está no presente ou no passado
*   dep: dependência sintática
*   shape: formato (maiúsculo, minúsculo, dígitos)
*   alpha: se é alfabético
*   stop:  se é stopword
"""

pln = spacy.load('pt') #Carregando os pacotes do spaCy instalados em português
pln

documento = pln('Estou aprendendo processamento da linguagem natural, curso no Rio de Janeiro')

for token in documento: #Obtendo os tokens, juntamente com as entidades de palavras correspondentes
  print(token.text, token.pos_)

for token in documento:
  print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)

for token in documento:
  if token.pos_ == 'PROPN': #Obtendo substantivos próprios
    print(token.text)

"""## Etapa 3: Lematização e stemização
*   Lematização: "Lema" de uma palavra de acordo com seu significado no dicionário - palavra base (análise vocabular e morfológica)
*   Stemização: Extrair o radical das palavras
"""

for token in documento:
  print(token.text, token.lemma_) #Obtendo a lematização de tokens

doc = pln('encontrei encontraram encontrarão encontrariam')
[token.lemma_ for token in doc]

"""## Comparação de stemização (NLTK) x lematização (spaCy)"""

!pip install nltk --upgrade

import nltk
nltk.download('rslp')

stemmer = nltk.stem.RSLPStemmer()
stemmer.stem('frequentou')

for token in documento:
  print(token.text, token.lemma_, stemmer.stem(token.text)) #Lematização + stemização

  #OBS: Não faz muito sentido extrair o radical de um substantivo próprio determinado pela lematização

"""## Etapa 4: Reconhecimento de entidades nomeadas
*   NER (Named-Entity Recognition)
*   Encontrar e classificar entidades no texto, dependendo da base de dados que foi utilizada para o treinamento (pessoa, localização, empresa, numéricos)
*   Usado em chatbots para saber o assunto falado
*   Siglas https://spacy.io/api/annotation#named-entities
"""

texto = 'Christopher Catesby Harington (Londres, 26 de dezembro de 1986) é um ator britânico. É mais conhecido por interpretar Jon Snow na série televisiva Game of Thrones da HBO.'

documento = pln(texto)
for entidade in documento.ents:
  print(entidade.text, entidade.label_)

from spacy import displacy
displacy.render(documento, style='ent', jupyter = True)

for entidade in documento.ents:
  if entidade.label_ == 'PER': #Obtendo entidades referentes às pessoas
    print(entidade.text)

"""## Etapa 5: Stopwords
* Palavras que aparecem com  muita frequência e que não apresentam muito significado (e, a, de, da, etc)
"""

from spacy.lang.pt.stop_words import STOP_WORDS
STOP_WORDS #Lista de stopwords em português - SpaCy

len(STOP_WORDS)

#Verificando se uma palavra é uma stopword
pln.vocab['ir'].is_stop

pln.vocab['caminhar'].is_stop

documento = pln('Christopher Catesby Harington (Londres, 26 de dezembro de 1986) é um ator britânico. É mais conhecido por interpretar Jon Snow na série televisiva Game of Thrones da HBO.')

#Listando todos os tokens que não são stopwords
for token in documento: 
  if not pln.vocab[token.text].is_stop:
    print(token.text)

"""## Etapa 6: Parsing de dependências

*  Relação pai-filho entre as palavras

### Exemplo 1
"""

documento = pln('reserve uma passagem saindo de Guarulhos e chegando no Rio de Janeiro')

list_locais = [entidade for entidade in documento.ents if entidade.label_ == 'LOC']
origem = list_locais[0][0]
destino = list_locais[-1][0]
origem, destino

list(origem.ancestors)

list(destino.ancestors)

"""### Exemplo 2"""

documento = pln('Reserva de uma mesa para o restaurante e de um táxi para o hotel')

tarefas = documento[3], documento[10]
locais = documento[6], documento[13]
# tarefas = [entidade for entidade in documento1.ents if entidade.label_ == 'PRODUCT']
# locais = [entidade for entidade in documento1.ents if entidade.label_ == 'FAC']

tarefas, locais

#Imprimindo a relação de dependência
for local in locais:
  print('-----', local)
  for objeto in local.ancestors:
    print(objeto)

for local in locais:
  for objeto in local.ancestors:
    if objeto in tarefas:
      print(f'A reserva de {objeto} é para o {local}')
      break

"""### Exemplo 3"""

#Imprimido o grafo de dependência
from spacy import displacy
documento = pln('Reserva de uma mesa para o restaurante e de um táxi para o hotel')
displacy.render(documento, style='dep', jupyter=True, options={'distance': 90})

list(documento[3].ancestors)

list(documento[3].children)

"""### Exemplo 4"""

documento = pln('Que locais podemos visitar em Gramado e para ficar em Canela?')
lugares = documento[5], documento[10]
acoes = documento[3], documento[8]
lugares, acoes

for local in lugares:
  for acao in local.ancestors:
    if acao in acoes:
      print(f'{local} para {acao}')
      break

displacy.render(documento, style='dep', jupyter=True, options={'distance': 90})

list(documento[3].children)

list(documento[3].ancestors)

list(documento[8].children)

"""##Etapa 7
  *   Verificar se duas palavras são semelhantes ou logicamente relacionadas
  *   Usar o algoritmo GloVe (Global Vectors for Word Representation)

###Exemplo 1
"""

p1 = pln('olá')
p2 = pln('oi')
p3 = pln('ou')

p1.similarity(p2) #Obtendo a probabilidade da similaridade de um termo com o outro

p1.similarity(p3)

p2.similarity(p3)

texto1 = pln('Quando será lançado o novo filme?')
texto2 = pln('O novo filme será lançado mês que vem')
texto3 = pln('Qual a cor do carro?')

texto1.similarity(texto2)

texto1.similarity(texto3)

"""##Exemplo 2"""

texto = pln('gato cachorro cavalo pessoa')
for texto1 in texto:
  print('-----', texto1)
  for texto2 in texto:
    # print(texto2)
    similaridade = int(texto1.similarity(texto2) * 100)
    print(f'{texto1} é {similaridade}% similar a {texto2}')

"""##Etapa 8: Tokenização"""

documento = pln('Estou aprendendo processamento da linguagem natural, curso no Rio de Janeiro')

for token in documento:
  print(token)

documento1 = 'Estou aprendendo processamento da linguagem natural, curso no Rio de Janeiro'
documento1.split(' ') #Outra forma de tokenizar: utilizar split(). Porém, não é recomendável pois terá situações em que alguns tokens que não representam uma palavra irão refletir no resultado do treinamento do algoritmo